# Host-Down Chaos Strategy

## Overview

The **host-down** chaos type simulates a complete node failure by **draining** the Nomad node, which forces all allocations to migrate to other healthy nodes in the cluster.

## Strategy: Node Drain

### What is Node Drain?

Node drain is a **graceful shutdown** mechanism in Nomad that:
1. Marks the node as **ineligible** for new allocations
2. Signals all running allocations to stop
3. Waits for allocations to shut down gracefully (up to deadline)
4. Triggers Nomad scheduler to place allocations on other nodes

### Why Drain Instead of Hard Kill?

| Approach | Description | Use Case |
|----------|-------------|----------|
| **Node Drain** âœ… | Graceful migration | Simulates planned maintenance, node upgrade |
| **Hard Kill** âŒ | Immediate termination | Simulates hardware failure, power loss |

We use **Node Drain** because:
- âœ… More realistic for most failure scenarios
- âœ… Allows graceful shutdown (saves state, closes connections)
- âœ… Nomad handles rescheduling automatically
- âœ… Reversible (can re-enable node)
- âœ… Production-safe (when done correctly)

## Implementation Strategy

### Step-by-Step Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. DISCOVER TARGET NODE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Find which node hosts the target service          â”‚
â”‚ â€¢ Get node ID, name, datacenter                     â”‚
â”‚ â€¢ Count allocations on node                         â”‚
â”‚                                                     â”‚
â”‚ Input:  service_id = "mobi-platform-account-service"â”‚
â”‚ Output: node_id = "538b4367-c20d-..."               â”‚
â”‚         node_name = "hostname"    â”‚
â”‚         alloc_count = 8                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â¬‡ï¸
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. VALIDATE NODE STATE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Check if node is already draining                 â”‚
â”‚ â€¢ Verify node status is "ready"                     â”‚
â”‚ â€¢ Warn about number of affected allocations         â”‚
â”‚                                                     â”‚
â”‚ Checks:                                             â”‚
â”‚ âœ“ Drain status = false                              â”‚
â”‚ âœ“ Status = ready                                    â”‚
â”‚ âš ï¸  ALL 8 allocations will be affected!             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â¬‡ï¸
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. ENABLE DRAIN MODE                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ POST to /v1/node/{id}/drain                       â”‚
â”‚ â€¢ Set drain deadline (default: 120 seconds)         â”‚
â”‚ â€¢ Mark node as ineligible                           â”‚
â”‚                                                     â”‚
â”‚ API Call:                                           â”‚
â”‚ {                                                   â”‚
â”‚   "DrainSpec": {                                    â”‚
â”‚     "Deadline": 120000000000,  // 120s in ns        â”‚
â”‚     "IgnoreSystemJobs": false                       â”‚
â”‚   },                                                â”‚
â”‚   "MarkEligible": false                             â”‚
â”‚ }                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â¬‡ï¸
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. NOMAD AUTOMATIC MIGRATION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Nomad Scheduler Takes Over:                         â”‚
â”‚ â€¢ Finds suitable nodes for each allocation          â”‚
â”‚ â€¢ Checks resource availability                      â”‚
â”‚ â€¢ Considers constraints and affinities              â”‚
â”‚ â€¢ Starts new allocations on target nodes            â”‚
â”‚ â€¢ Signals old allocations to stop                   â”‚
â”‚                                                     â”‚
â”‚ Timeline:                                           â”‚
â”‚ 0-10s:  New allocations placed                      â”‚
â”‚ 10-30s: New instances starting                      â”‚
â”‚ 30-60s: Health checks passing                       â”‚
â”‚ 60-90s: Old allocations stopping                    â”‚
â”‚ 90s+:   Migration complete                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â¬‡ï¸
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. VERIFY DRAIN STATUS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Wait 3 seconds for API to reflect changes         â”‚
â”‚ â€¢ Query node status                                 â”‚
â”‚ â€¢ Confirm drain is active                           â”‚
â”‚ â€¢ Check scheduling eligibility                      â”‚
â”‚                                                     â”‚
â”‚ Expected:                                           â”‚
â”‚ âœ“ Drain = true                                      â”‚
â”‚ âœ“ SchedulingEligibility = "ineligible"             â”‚
â”‚ âœ“ Status = "ready" (still healthy)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â¬‡ï¸
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. PROVIDE RECOVERY INSTRUCTIONS                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âš ï¸  MANUAL INTERVENTION REQUIRED!                   â”‚
â”‚                                                     â”‚
â”‚ To re-enable node after testing:                    â”‚
â”‚ $ nomad node eligibility -enable {node_id}          â”‚
â”‚                                                     â”‚
â”‚ Or via API:                                         â”‚
â”‚ $ curl -X POST http://nomad:4646/v1/node/{id}/...  â”‚
â”‚    -d '{"Eligibility":"eligible"}'                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Configuration Parameters

### 1. Drain Deadline

```python
duration_seconds = 120  # Default: 2 minutes
```

**What it means:**
- Maximum time for graceful shutdown
- After deadline, remaining allocations are **force-killed**
- Longer deadline = more time for graceful migration
- Shorter deadline = faster chaos but may cause abrupt termination

**Recommendations:**
```python
# Fast-starting services (stateless web apps)
duration_seconds = 60

# Database services with state
duration_seconds = 300

# Mission-critical with complex shutdown
duration_seconds = 600
```

### 2. IgnoreSystemJobs

```python
"IgnoreSystemJobs": False  # Drain ALL jobs including system jobs
```

**Options:**
- `False`: Drain everything (default) - most realistic
- `True`: Keep system jobs running - less disruptive

**System jobs include:**
- Log collectors
- Monitoring agents
- Security scanners
- Infrastructure services

### 3. MarkEligible

```python
"MarkEligible": False  # Node stays ineligible after drain
```

**What it means:**
- `False`: Node remains drained until manual re-enable âœ…
- `True`: Node automatically becomes eligible after drain

**Why False?**
- Prevents automatic re-population
- Gives time to analyze impact
- Requires conscious recovery decision
- Simulates permanent node loss

## How Nomad Handles Migration

### Scheduler Logic

```
FOR each allocation on draining node:
  1. Find eligible nodes with capacity
  2. Calculate placement scores based on:
     - Available resources
     - Existing allocations (spread)
     - Constraints (datacenter, class)
     - Affinities (prefer certain nodes)
  3. Select best candidate node
  4. Create new allocation on target node
  5. Wait for new allocation to be healthy
  6. Signal old allocation to stop (SIGTERM)
  7. Wait for graceful shutdown (up to deadline)
  8. Force kill if deadline exceeded (SIGKILL)
```

### Example Migration Flow

```
Before Drain:
  msacc01p1: [Service-A, Service-B, Service-C]  â† Draining
  msacc02p1: [Service-D]
  msacc03p1: [Service-E]

During Drain:
  msacc01p1: [Service-A*, Service-B*, Service-C*]  â† Stopping
  msacc02p1: [Service-D, Service-Aâ†“]               â† Starting
  msacc03p1: [Service-E, Service-Bâ†“, Service-Câ†“]   â† Starting

After Drain:
  msacc01p1: []                                     â† Empty
  msacc02p1: [Service-D, Service-A]                 â† Running
  msacc03p1: [Service-E, Service-B, Service-C]      â† Running
```

## Strategy Advantages

### âœ… Pros

1. **Realistic**
   - Mimics real-world maintenance scenarios
   - Similar to node upgrades, decommissioning
   - Tests actual failure handling paths

2. **Controlled**
   - Graceful shutdown (saves state)
   - Predictable timeline
   - Reversible (can re-enable)

3. **Comprehensive**
   - Tests service redundancy
   - Validates cluster capacity
   - Exercises Nomad scheduler
   - Checks health checks and readiness

4. **Production-Safe**
   - No data corruption risk
   - Nomad handles complexity
   - Can be scheduled during maintenance windows

5. **Observable**
   - Can watch migration in real-time
   - Clear status indicators
   - Detailed API responses

### âš ï¸ Cons

1. **Not Instant**
   - Takes 1-3 minutes for full migration
   - Doesn't simulate instant hardware failure

2. **Requires Capacity**
   - Cluster must have spare resources
   - May fail if no suitable nodes available

3. **Affects All Services**
   - Can't selectively drain one service
   - High blast radius

4. **Manual Recovery**
   - Requires explicit re-enable command
   - Easy to forget and leave node drained

5. **Permission Requirements**
   - Needs `node { policy = "write" }` ACL
   - Not available with basic tokens

## Alternative Strategies (Not Implemented)

### Strategy 2: Allocation Stop

```python
# Stop specific allocation instead of draining node
nomad.allocation.stop(allocation_id)
```

**Pros:**
- âœ… Surgical (affects only target service)
- âœ… Lower blast radius

**Cons:**
- âŒ Less realistic (nodes don't fail partially)
- âŒ Doesn't test cluster-wide impact

### Strategy 3: Node Eligibility Toggle

```python
# Mark node ineligible without draining
nomad.node.set_eligibility(node_id, eligible=False)
```

**Pros:**
- âœ… Prevents new allocations
- âœ… Keeps existing allocations running

**Cons:**
- âŒ Doesn't simulate failure (services keep running)
- âŒ Not a realistic failure mode

### Strategy 4: Job Stop

```python
# Stop the job entirely
nomad.job.deregister(job_id)
```

**Pros:**
- âœ… Simple and direct

**Cons:**
- âŒ Affects all instances, not just one node
- âŒ Too aggressive for chaos testing

## Monitoring the Strategy

### What to Watch

```bash
# Terminal 1: Monitor node status
watch -n 2 'nomad node status {node_id} | head -30'

# Terminal 2: Monitor allocations
watch -n 2 'nomad node status {node_id} | grep -A 20 Allocations'

# Terminal 3: Monitor job status
watch -n 2 'nomad job status {service_id}'

# Terminal 4: Monitor cluster capacity
watch -n 5 'chaosmonkey discover --clients'
```

### Key Metrics

| Metric | Normal | During Drain | What to Check |
|--------|--------|--------------|---------------|
| **Drain Status** | false | true | Node API |
| **Eligibility** | eligible | ineligible | Node API |
| **Allocation Count** | 8 | 8â†’6â†’4â†’2â†’0 | Node status |
| **Service Instances** | 1 | 1â†’2â†’1 | Job status |
| **Response Time** | 50ms | 100-500ms | APM/logs |
| **Error Rate** | 0.1% | 2-5% | Monitoring |

## Recovery Strategy

### Automatic Recovery (Not Implemented)

Ideally, we would:
```python
# Schedule a job to re-enable node after duration
def schedule_node_recovery(node_id, delay_seconds):
    time.sleep(delay_seconds)
    nomad.node.set_eligibility(node_id, eligible=True)
    nomad.node.drain_node(node_id, enable=False)
```

### Manual Recovery (Current Approach)

**Step 1: Check node is empty**
```bash
nomad node status {node_id} | grep "Allocations"
# Should show 0 allocations
```

**Step 2: Re-enable node**
```bash
# Disable drain
nomad node drain -disable {node_id}

# Make eligible again
nomad node eligibility -enable {node_id}
```

**Step 3: Verify recovery**
```bash
nomad node status {node_id}
# Should show:
# Drain: false
# Eligibility: eligible
# Status: ready
```

**Step 4: Monitor repopulation**
```bash
watch -n 5 'nomad node status {node_id}'
# Allocations should gradually return as scheduler rebalances
```

## Comparison with Other Strategies

| Strategy | Realism | Control | Blast Radius | Reversible | Complexity |
|----------|---------|---------|--------------|------------|------------|
| **Node Drain** | â­â­â­â­â­ | â­â­â­â­ | ğŸ”´ High | âœ… Yes | Medium |
| Allocation Stop | â­â­â­ | â­â­â­â­â­ | ğŸŸ¡ Low | âœ… Yes | Low |
| Node Ineligible | â­â­ | â­â­â­â­â­ | ğŸŸ¢ None | âœ… Yes | Low |
| Job Stop | â­â­ | â­â­â­ | ğŸ”´ Very High | âœ… Yes | Low |
| Hard Kill | â­â­â­â­â­ | â­ | ğŸ”´ High | âŒ No | High |

## Best Practices

### Before Running

1. âœ… **Verify cluster capacity**
   ```bash
   chaosmonkey discover --clients
   # Ensure other nodes can absorb workload
   ```

2. âœ… **Check service redundancy**
   ```bash
   nomad job status {service_id}
   # Ensure count > 1
   ```

3. âœ… **Coordinate with team**
   - Announce in chat
   - Get approval
   - Set up monitoring

### During Chaos

1. âœ… **Watch migration progress**
   ```bash
   watch -n 2 'nomad node status {node_id}'
   ```

2. âœ… **Monitor service health**
   - Check APM dashboard
   - Watch error rates
   - Monitor response times

3. âœ… **Document observations**
   - Note migration duration
   - Record any issues
   - Capture screenshots

### After Chaos

1. âœ… **Verify migration complete**
   ```bash
   nomad node status {node_id}
   # Should show 0 allocations
   ```

2. âœ… **Re-enable node**
   ```bash
   nomad node eligibility -enable {node_id}
   ```

3. âœ… **Validate service recovered**
   ```bash
   nomad job status {service_id}
   # All instances should be healthy
   ```

4. âœ… **Review and document**
   - Write incident report
   - Note lessons learned
   - Update runbooks

## Summary

The **Node Drain** strategy provides:
- ğŸ¯ **Realistic** failure simulation
- ğŸ”’ **Safe** and reversible process
- ğŸ“Š **Observable** migration process
- âš™ï¸ **Automated** by Nomad scheduler
- âš ï¸ **High impact** for thorough testing

**Trade-off:** Requires elevated permissions and manual recovery, but provides the most comprehensive test of cluster resilience.

---

**Strategy Status**: âš ï¸ Implemented but requires `node { policy = "write" }` ACL permissions

**Documentation**: See `docs/NODE_DRAIN.md` for complete usage guide
